{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #toolbox to work with dataframes\n",
    "import numpy as np #toolbox to work with narrays\n",
    "import matplotlib.pyplot as plt #toolbox to do plots\n",
    "from sklearn.svm import SVC #load the support vector machine model functions\n",
    "from sklearn.model_selection import train_test_split #load the function to split train and test sets\n",
    "from sklearn import metrics # get the report\n",
    "from sklearn.metrics import classification_report # get the report\n",
    "from sklearn import preprocessing # normalize the features\n",
    "from sklearn.preprocessing import MinMaxScaler # normalize the features\n",
    "from sklearn.feature_selection import SelectKBest #load the feature selector model  \n",
    "from sklearn.feature_selection import chi2 #feature selector algorithm\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalized_data (df,t):\n",
    "\n",
    "    if (t==1):\n",
    "        d=df.copy() # min max normalization\n",
    "        for each_collum in range(0,df.shape[1]):\n",
    "            max =df.iloc[:,each_collum].max()\n",
    "            min =df.iloc[:,each_collum].min()\n",
    "            d.iloc[:,each_collum]=(d.iloc[:,each_collum]-min)/(max-min)\n",
    "    elif (t==2):\n",
    "        d=df.copy() # mean normalization\n",
    "        for each_collum in range(0,df.shape[1]):\n",
    "            max =df.iloc[:,each_collum].max()\n",
    "            min =df.iloc[:,each_collum].min()\n",
    "            mean =df.iloc[:,each_collum].mean()\n",
    "            d.iloc[:,each_collum]=(d.iloc[:,each_collum]-mean)/(max-min)\n",
    "    \n",
    "    else:\n",
    "        d=df.copy() # standardization\n",
    "        for each_collum in range(0,df.shape[1]):\n",
    "            mean =df.iloc[:,each_collum].mean()\n",
    "            std =df.iloc[:,each_collum].std()\n",
    "            d.iloc[:,each_collum]=(d.iloc[:,each_collum]-mean)/(std)\n",
    "\n",
    "    return d\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the function to performe feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "#load the feature selection algorithms\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import SelectFpr\n",
    "from sklearn.feature_selection import SelectFdr\n",
    "from sklearn.feature_selection import SelectFwe\n",
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "\n",
    "# function to do feature selection\n",
    "def feature_selector(X_train,y_train,type,i):\n",
    "    if (type == 1):\n",
    "#ANOVA F-value between label/feature for classification tasks.\n",
    "        bestfeatures = SelectKBest(score_func = f_classif, k=i)\n",
    "    elif(type == 2):\n",
    "#Mutual information for a discrete target.\n",
    "        bestfeatures = SelectKBest(score_func=mutual_info_classif, k=i)\n",
    "    elif(type == 3):\n",
    "    #Chi-squared stats of non-negative features for classification tasks.\n",
    "        bestfeatures = SelectKBest(score_func=chi2, k=i)\n",
    "    elif(type == 4):\n",
    "#Select features based on an estimated false discovery rate.\n",
    "        bestfeatures = SelectKBest(score_func=SelectFdr, k=i)\n",
    "    elif(type == 5):\n",
    "#Select features based on family-wise error rate.\n",
    "        bestfeatures = SelectKBest(score_func=SelectFwe, k=i)\n",
    "#Perform the feature based on selected algorithm\n",
    "    fit = bestfeatures.fit(X_train,y_train)\n",
    "    cols_idxs = fit.get_support(indices=True)\n",
    "    Xt=X_train.iloc[:,cols_idxs] # extract the best features for training\n",
    "    return Xt,cols_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd step - load and design the classifiers\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,ExtraTreesClassifier,RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "classifiers = [\n",
    "    SVC(gamma='auto',probability=True),\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegression(solver='lbfgs'),\n",
    "    BaggingClassifier(),\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    AdaBoostClassifier(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    OneVsRestClassifier(LinearSVC(random_state=0)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from scipy import interp\n",
    "def cross_validation(model, _X, _y):\n",
    "    scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "       'precision' : make_scorer(precision_score, average='macro'),\n",
    "       'recall' : make_scorer(recall_score, average='macro'), \n",
    "       'f1_score' : make_scorer(f1_score, average='macro')}\n",
    "    #_scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "    stratified_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    #kfold = model_selection.LeaveOneOut()  \n",
    "    results = cross_validate(estimator=model,\n",
    "                               X=_X,\n",
    "                               y=_y,\n",
    "                               cv=stratified_cv,\n",
    "                               scoring=scoring,\n",
    "                               return_train_score=True)\n",
    "    \n",
    "    return [results['train_accuracy'].mean()*100, results['train_precision'].mean(), results['train_recall'].mean(), results['train_f1_score'].mean(),results['test_accuracy'].mean()*100,results['test_precision'].mean(), results['test_recall'].mean(),results['test_f1_score'].mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Camellia_Data.xlsx'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[1;32mIn[12], line 5\u001b[0m\n    df = pd.read_excel('Camellia_Data.xlsx',sheet_name=u)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:504\u001b[0m in \u001b[1;35mread_excel\u001b[0m\n    io = ExcelFile(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:1563\u001b[0m in \u001b[1;35m__init__\u001b[0m\n    ext = inspect_excel_format(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:1419\u001b[0m in \u001b[1;35minspect_excel_format\u001b[0m\n    with get_handle(\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:872\u001b[0;36m in \u001b[0;35mget_handle\u001b[0;36m\n\u001b[0;31m    handle = open(handle, ioargs.mode)\u001b[0;36m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m\u001b[0;31m:\u001b[0m [Errno 2] No such file or directory: 'Camellia_Data.xlsx'\n"
     ]
    }
   ],
   "source": [
    "t=['NIR','FTIR']\n",
    "\n",
    "for u in t:\n",
    "\n",
    "    df = pd.read_excel('Camellia_Data.xlsx',sheet_name=u) \n",
    "    target=df.iloc[:,-1]\n",
    "    df=df.iloc[:,:-1]\n",
    "    df.index=df['Unnamed: 0']\n",
    "    df=df.drop('Unnamed: 0',axis=1)\n",
    "    d_n=normalized_data (df.astype('Float64'),3)\n",
    "    perf_results=pd.DataFrame()\n",
    "    o=0\n",
    "    for k in range(1,30,1):\n",
    "        pcadata = PCA(n_components=k)\n",
    "        for j in tqdm(range(30,d_n.shape[1],10)):    \n",
    "            df_nf,cols_idxs=feature_selector(d_n,target,1,j)\n",
    "            principalComponents = pcadata.fit_transform(df_nf)\n",
    "            for i in classifiers:\n",
    "                a,b,c,d,e,f,g,h=cross_validation(i,pd.DataFrame(principalComponents), target)\n",
    "                perf_results[o]=[k,j,df.columns[cols_idxs],i,a,b,c,d,e,f,g,h]\n",
    "                o=o+1       \n",
    "        perf_results=perf_results.T\n",
    "        perf_results.columns=['Pca_number_feat','# features','features','classifier',\"Mean Training Accuracy\",\"Mean Training Precision\",\"Mean Training Recall\",\"Mean Training F1 Score\",\"Mean Validation Accuracy\",\"Mean Validation Precision\",\"Mean Validation Recall\", \"Mean Validation F1 Score\"]\n",
    "        perf_results.to_excel(f'/resultadosPCA{k}_{u}.xlsx')  \n",
    "        perf_results=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f681d13594f6d13fd384c1eed34ffbd142ac2c86c0f865443692fcf61f9efdbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
